\chapter{Detailed Description and Results}
\label{sec:results-int}

\section{Fulgor AI Project}

The Fulgor AI project aimed to develop an AI-powered solution for predicting
the quality of produced cable splices using IoT time series data from cable
production processes. This project was essential in helping the client optimize
their production process, reduce costs, and improve overall product quality.

The primary goal of the Fulgor AI project was to provide an end-to-end (E2E) AI
solution that could predict coil quality and identify the factors affecting it.
The business value of this project included reducing production costs by
identifying early signals of low-quality coils, reducing manual effort by
automating the identification of factors affecting quality, and reducing
downtime by alerting production teams to potential issues in real-time.

For this project, we were a group of three data scientists, one being the
manager and me and my colleague being the team members. We were responsible for
collecting, preprocessing, and analyzing the data, developing predictive models
to forecast coil quality, and providing actionable insights to the production
team, by working closely with the production team and the IT department, with
the assistance and guidance of our supervisor.

\subsection{Data Collection}

The data collection process involved gathering various datasets from multiple
sources provided by the client after many discussions and meetings about what
is needed and how to collect it. We received historian data on 2 April 2024,
which was later supplemented with additional data containing missing tags on 15
April 2024. We also collected Factory Talk data on 8 April 2024, flipchart data
on 8 April 2024, and Excel `ΠΑΡΑΓΩΓΗ` data on 15 April 2024. The historian data
contained information on the production process, while the Factory Talk data
provided details on the duration of the process. The flipchart and Excel data
contained labels for the quality of the produced coils. The data was collected
in a structured format to facilitate further analysis and modeling tasks. We
received most of the data in zipped parquet files that were from November 2022
to March 2024, which were then extracted and processed for further analysis.
The data collection process was crucial for ensuring that we had the necessary
information to build accurate predictive models for coil quality, and it laid
the foundation for the subsequent data preprocessing and analysis steps.

\subsection{Exploratory Data Analysis (EDA)}

The exploratory data analysis (EDA) phase was crucial for understanding the
given data and identifying patterns, trends, and anomalies that could provide
insights into the quality of the cable splices. During the EDA phase, we
firstly discovered that the data had many missing values along with duplicate
or missing CoilNums of cable productions. We also observed that the quality
distribution was highly imbalanced, with a majority of coils having a quality
of label \emph{0.2} out of a possible range of \emph{0.2, 0.4, 1, $>1$, ΑΠΟΡ}.
Specifically, the quality distribution was as follows: \emph{0.2} - 5761 coils,
\emph{0.4} - 1102 coils, \emph{1} - 864 coils, `\emph{$>1$}` - 542 coils, and
\emph{ΑΠΟΡ} - 42 coils. The best quality was \emph{0.2} and the quality
decreased as the label increased, with \emph{ΑΠΟΡ} being the worst quality,
which meant that the cable was to be scrapped. The quality labels represented
the thickness of the cable splices, with \emph{0.2} being the lowest thickness
and \emph{ΑΠΟΡ} being the highest thickness. Additionally, the production data
had approximately 2 million rows and 245 columns, meaning that the quality of
the cable splices was influenced by a large number of factors, which needed to
be identified and analyzed to build accurate predictive models.

During the EDA phase, we also visualized the data using various techniques such
as histograms, ridge and box plots, and correlation matrices to identify
patterns and relationships between different variables as well as time series
plots to observe trends over time. We observed that the data had a significant
amount of missing values, which needed to be addressed during the data
preprocessing phase and that the data were not as expected by the client, in
terms of the distribution of many features, which needed to be further
investigated, where we later found out that some of the data were not collected
properly by the client due to errors of the sensors.

Specifically for the statistical analysis, we created a correlation matrix to
understand relationships between different features, displaying Pearson
correlation coefficients between pairs of features to highlight potential
multicollinearity issues. Time series plots were generated for key variables to
observe trends over time, including line plots and seasonal decomposition plots
to identify any seasonal patterns or trends. Additionally, statistical tests
such as the Augmented Dickey-Fuller (ADF) test were conducted to check for
stationarity in the time series data.

We also identified several key features that could potentially impact the
quality of the cable splices, such as production time, temperature, pressure,
and other process variables, by summarizing the dataset to understand its
structure and basic statistics, including mean, median, standard deviation,
minimum, and maximum values for each feature, where we noticed that the
variance of the features could be a key factor affecting the quality of the
cable splices. The EDA phase provided valuable insights into the structure and
quality of the data, which guided the subsequent data preprocessing and feature
engineering steps.

\subsection{Data Preprocessing}

The data preparation and preprocessing steps included joining the historian
data, filtering Factory Talk data by duration (12-18 minutes), removing
duplicate CoilNums from Factory Talk, retrieving CoilNums based on historian
data timestamps, joining flipchart and Excel `Παραγωγή` to obtain labels, and
merging historian data with quality and labels. We also removed \emph{ΑΠΟΡ}
labels, filtered out outlier values, and excluded columns with high occurrences
of missing values and zeros. Additionally, we cleaned invalid values and
created categorical labels for the quality of the coils. After, again
communicating with the production team and the IT department, we kept only the
columns that were known to impact quality.

The data preprocessing phase also included the feature engineering process,
where we extracted relevant features from the raw data to improve the accuracy
of the predictive models. We used domain knowledge and specialized time series
feature extraction techniques to identify key indicators of cable coil quality.
We employed the \emph{tsfresh} library to extract a wide range of features from
the time series data, including statistical features such as kurtosis and
skewness, time series features, and feature selection techniques. The feature
engineering process was essential for transforming the raw data into a suitable
format for machine learning algorithms and improving the predictive performance
of the models.

\subsection{Initial Experiments}

Time series analysis in industrial settings often involves large amounts of
data that need to be processed, analyzed, and used for predictive tasks. The
project focused on transforming a time series problem into a classification
problem using \emph{tsfresh} for feature extraction. This approach differs from
traditional forecasting.

In our initial experiments, we tested various approaches to predicting coil
quality and tracked them using \emph{MLflow} to monitor the performance of the
models. We experimented with different classification algorithms, including
Random Forest, Gradient Boosting, and Neural Networks, to determine the most
effective approach for predicting coil quality. We also used cross-validation
techniques to evaluate the performance of the models and optimize their
hyperparameters. The initial experiments provided valuable insights into the
predictive performance of the models and guided the development of more
sophisticated models in subsequent experiments.

\textbf{Experiment 1: Predicting Value ΟΞΕΙΔΙΑ}
\begin{itemize}
    \item Initial Goal: Predict the value of ΟΞΕΙΔΙΑ.
    \item Limitations: Highly skewed target values.
    \item Outcome: Low accuracy led to further exploration.
\end{itemize}

\textbf{Experiment 2: Predicting Categorical Label (ΜΕΤΡΗΣΗ 4)}
\begin{itemize}
    \item Goal: Predict the categorical label ΜΕΤΡΗΣΗ 4.
    \item Limitations: Class imbalance.
    \item Results: Insufficient data to determine the significance of features for
          prediction.
\end{itemize}

\textbf{Experiment 3: Predicting Quality Sum}
\begin{itemize}
    \item Goal: Predict Quality Sum: \emph{0.2, 0.4, 1, $>1$, ΑΠΟΡ}.
    \item Limitations: Class imbalance.
    \item Results: No evident differences in class prediction.
\end{itemize}

\textbf{Experiment 4: Binary Classification}
\begin{itemize}
    \item Approach: Classify as `Good` \emph{(0.2)} and `Not Good` \emph{(1, `$>1$`,
              ΑΠΟΡ)} and drop \emph{0.4} coils due to ambiguity with `Good`, as they were
          barely distinguishable.
    \item Limitations: Class imbalance.
    \item Rationale: Refined previous experiments based on insights gained.
\end{itemize}

\subsection{Grid Search and Cross-Validation}

After the initial experiments, we focused on binary classification to predict
the quality of the produced cable splices, where this time, after communicating
with the production team and the IT department, we included the \emph{0.4}
quality, so no coils would be left out. We implemented grid search and
cross-validation to optimize model performance. Several classification
algorithms were used, including \emph{XGBClassifier},
\emph{HistGradientBoostingClassifier}, \emph{LGBMClassifier}, and
\emph{CatBoostClassifier}, by utilizing the \emph{sklearn} and
\emph{imbalanced-learn} libraries for the models. We employed
\emph{TimeSeriesSplit} cross-validation to respect the temporal order of the
data and prevent data leakage. Grid search was used for hyperparameter tuning
to find the best-performing model. Supplementing the grid search for the
optimal hyperparameters, we used \emph{Optuna} to further optimize the
hyperparameters of the models and we also applied Grid Search for many scaling
techniques, such as \emph{StandardScaler}, \emph{MinMaxScaler},
\emph{RobustScaler}, and \emph{PowerTransformer}, along with many sampling
techniques, such as \emph{SMOTE}, \emph{SMOTEENN}, and \emph{SMOTETomek}. The
models were evaluated based on various metrics, including accuracy, precision,
recall, F1-score, and AUC-ROC score.

The best-performing model was selected based on the AUC score, which provided a
comprehensive evaluation of the model's performance across different
thresholds, which also accounted for the class imbalance in the dataset, since
the AUC score is not affected by class imbalance like accuracy is. After
developing the models, we used \emph{SHAP} to explain the model predictions and
identify the key features that influenced the quality of the cable splices. For
the evaluation of the results, the interpretability of the model was crucial,
as it provided valuable insights into the factors affecting coil quality which
could be used to improve the production process. Hence, the interpretability of
the \emph{SHAP} diagram was crucial for the stakeholders to understand the
model's predictions and make informed decisions based on the insights provided
and that is the reason why more complex techniques such as Neural Networks were
not used, as they are not as interpretable as the other models used. Below, we
can see the Experiments that were conducted, in the Grid Search and
Cross-Validation phase.

\textbf{Experiment 5: Multiclass Classification}
\begin{itemize}
    \item Tried predicting all qualities present in the validation dataset.
    \item Despite oversampling with SMOTE, the imbalanced dataset presented challenges.
\end{itemize}

\textbf{Experiment 6: Binary Classification with Grid Search and Time Series Cross-Validation}
\begin{itemize}
    \item Evaluated models on validation data from April 2024.
    \item Used SHAP to explain model predictions.
\end{itemize}

\textbf{Experiment 7: Including Quality 0.4 in Binary Classification}
\begin{itemize}
    \item Included coils with a quality sum of 0.4 in the `good` category.
    \item Improved model performance.
\end{itemize}

\textbf{Experiment 8: Applying Business Rule provided by the client}
\begin{itemize}
    \item Changed model's prediction to `bad` if the two previous coils were predicted as
          `bad`.
    \item Improved predictive performance even further.
\end{itemize}

\subsection{Validation}

For the validation phase, we applied the same preprocessing steps used for the
training dataset. However, we only retained the selected features chosen during
the training dataset preprocessing. Invalid values were removed to ensure the
quality of the validation dataset. Moreover, the validation dataset was where
the coils produced in April 2024, which were not used for training the models.
The validation dataset had a shape of 53.226 rows and 205 columns, with 559
unique CoilNums. The value counts for binary classification were 423 `Good` and
91 `Bad` coils. The validation dataset was necessary to not include data that
the models had seen before, in order to effectively validate the models to
ensure their accuracy and reliability in predicting the quality of the produced
cable splices.

After all the previous experiments, we conducted the final experiment, which
was the most successful one, where we included the quality of the previous two
coils as additional features, which significantly improved model performance,
achieving an AUC score of 76\%. This approach leveraged the temporal nature of
the data and captured the sequential relationship between coils, which was
crucial for predicting coil quality accurately. The final model was able to
predict the quality of the produced cable splices with high accuracy,
considering the imbalanced nature of the dataset and the temporal dependencies
between coils. The model was able to identify some key indicators of coil
quality such as the variance as we had previously observed during the EDA
phase, which was crucial for improving the production process and reducing
costs.

\subsection{Conclusion}

The Fulgor AI project successfully developed and delivered predictive models
that accurately forecasted the quality of produced cable splices. Key
achievements included implementing advanced data preprocessing and feature
engineering techniques, creating comprehensive data visualizations and reports,
and using model explainability techniques to derive meaningful insights.
Incorporating the quality of previous coils as additional features
significantly improved model performance at the end, leaving the client with a
highly accurate and reliable AI-powered solution for predicting coil quality,
that was later also presented at an energy conference in Athens, Greece in May
2024.

Overall, the project provided valuable experience in applying data science and
machine learning techniques to solve real-world problems, contributing to both
my professional growth and the client's operational improvements.
