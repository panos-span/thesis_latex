\chapter{Detailed Description and Results}
\label{sec:results-int}

\section{Cable Coil Prediction AI Project}

The Cable Coil Prediction AI Project aimed to develop an AI-powered solution
for predicting the quality of produced cable coils using IoT time series data
from cable production processes. This project was essential in helping the
client optimize their production process, reduce costs, and improve overall
product quality.

The primary goal of the project was to provide an end-to-end (E2E) AI solution
that could predict coil quality and identify the factors affecting it. The
business value of this project included reducing production costs by
identifying early signals of low-quality coils, reducing manual effort by
automating the identification of factors affecting quality, and reducing
downtime by alerting production teams to potential issues in real-time.

For this project, we were a group of three data scientists, one being the
manager and me and my colleague being the team members. We were responsible for
collecting, preprocessing, and analyzing the data, developing predictive models
to forecast coil quality, and providing actionable insights to the production
team, by working closely with the production team and the IT department, with
the assistance and guidance of our supervisor.

\section{Data Collection}

The data collection process involved gathering various datasets from multiple
sources provided by the client after many discussions and meetings about what
is needed and how to collect it. We received historian data on 2 April 2024,
which was later supplemented with additional data containing missing tags on 15
April 2024. We also collected Factory data on 8 April 2024, flipchart data on 8
April 2024, and prdouction data on 15 April 2024. The historian data contained
information on the production process, while the Factory Talk data provided
details on the duration of the process. The flipchart and production data
contained labels for the quality of the produced coils. The data was collected
in a structured format to facilitate further analysis and modeling tasks. We
received most of the data in zipped parquet files that were from November 2022
to March 2024, which were then extracted and processed for further analysis.
The data collection process was crucial for ensuring that we had the necessary
information to build accurate predictive models for coil quality, and it laid
the foundation for the subsequent data preprocessing and analysis steps. For
this task, we used Python libraries such as \emph{Pandas} and \emph{Dask} to
handle the data and extract the necessary information for the analysis, where
my colleague was primarily responsible for the streamlining of the data
collection and normalization process.

\section{Exploratory Data Analysis (EDA)}

The exploratory data analysis (EDA) phase was crucial for understanding the
given data and identifying patterns, trends, and anomalies that could provide
insights into the quality of the cable coils, which was a task that I had been
assigned to do. The qualities were 5 labels, with the first one being the best
quality and the last one being the worst quality, with the quality decreasing
as we moved from the first to the last label, where the last label meant that
the cable was to be scrapped. The quality labels represented the thickness of
the cable coils, the best being the lowest thickness and the worst being the
highest thickness.

During the EDA phase, we firstly discovered that the data had many missing
values along with duplicate or missing CoilNums of cable productions. We also
observed that the quality distribution was highly imbalanced. Specifically, the
quality distribution was as follows: \emph{1st} - 5761 coils, \emph{2nd} - 1102
coils, \emph{3rd} - 864 coils, \emph{4th} - 542 coils, and \emph{5th} - 42
coils. Additionally, the production data had approximately 2 million rows and
245 columns, meaning that the quality of the cable coils was influenced by a
large number of factors, which needed to be identified and analyzed to build
accurate predictive models.

During the EDA phase, we also visualized the data using various techniques such
as histograms, ridge and box plots, and correlation matrices to identify
patterns and relationships between different variables as well as time series
plots to observe trends over time. We observed that the data had a significant
amount of missing values, which needed to be addressed during the data
preprocessing phase and that the data were not as expected by the client, in
terms of the distribution of many features, which needed to be further
investigated, where we later found out that some of the data were not collected
properly by the client due to errors of the sensors.

Specifically for the statistical analysis, we created a correlation matrix to
understand relationships between different features, displaying Pearson
correlation coefficients between pairs of features to highlight potential
multicollinearity issues. Time series plots were generated for key variables to
observe trends over time, including line plots and seasonal decomposition plots
to identify any seasonal patterns or trends. Additionally, statistical tests
such as the Augmented Dickey-Fuller (ADF) test were conducted to check for
stationarity in the time series data.

We also identified several key features that could potentially impact the
quality of the cable coils, such as production time, temperature, pressure, and
other process variables, by summarizing the dataset to understand its structure
and basic statistics, including mean, median, standard deviation, minimum, and
maximum values for each feature, where we noticed that the variance of the
features could be a key factor affecting the quality of the cable coils. The
EDA phase provided valuable insights into the structure and quality of the
data, which guided the subsequent data preprocessing and feature engineering
steps.

\section{Data Preprocessing}

The data preparation and preprocessing steps included joining the historian
data, filtering Factory Talk data by duration (12-18 minutes), removing
duplicate CoilNums from Factory Talk, retrieving CoilNums based on historian
data timestamps, joining flipchart and production data to obtain labels, and
merging historian data with quality and labels. We also removed the last label,
filtered out outlier values, and excluded columns with high occurrences of
missing values and zeros. Additionally, we cleaned invalid values and created
categorical labels for the quality of the coils. After, again communicating
with the production team and the IT department, we kept only the columns that
were known to impact quality.

The data preprocessing phase also included the feature engineering process,
where we extracted relevant features from the raw data to improve the accuracy
of the predictive models. We used domain knowledge and specialized time series
feature extraction techniques to identify key indicators of cable coil quality.
We employed the \emph{tsfresh} library to extract a wide range of features from
the time series data, including statistical features such as kurtosis and
skewness, time series features, and feature selection techniques. The feature
engineering process was essential for transforming the raw data into a suitable
format for machine learning algorithms and improving the predictive performance
of the models.

For this phase, me and my collegue decided to split the work, where I was
responsible for the data cleaning and feature engineering process, while my
colleague was responsible for the data preprocessing and the creation of the
categorical labels for the quality of the coils.

\section{Initial Experiments}

Time series analysis in industrial settings often involves large amounts of
data that need to be processed, analyzed, and used for predictive tasks. The
project focused on transforming a time series problem into a classification
problem using \emph{tsfresh} for feature extraction. This approach differs from
traditional forecasting.

In my initial experiments, I tested various approaches for predicting coil
quality and tracked them using \emph{MLflow} to monitor the performance of the
models. In order to perform the tracking of the experiments and gain access to
the AzureML platform, I used \emph{Bash} and \emph{SSH} keys to connect to the
platform and run the experiments, which I was not familiar with, but I managed
to learn quickly and adapt to the new environment, thanks to the guidance of my
colleague and the supervisor.

I experimented with different classification algorithms, including Random
Forest, Gradient Boosting, and Neural Networks by utilizing the Python
libraries \emph{sklearn}, \emph{imbalanced-learn} and \emph{tensorflow}, to
determine the most effective approach for predicting coil quality. I also used
cross-validation techniques to evaluate the performance of the models and
optimize their hyperparameters. The initial experiments provided valuable
insights into the predictive performance of the models and guided the
development of more sophisticated models in subsequent experiments.

Important to also note is that the first two initial experiments were conducted
with the intention of checking the feasibility of predicting the quality of the
cable coils from firstly predicting the value of 13 different features using
Regression techniques by using different metrics, such as \emph{RMSE, MAE, and
    R2}, and different models such as Linear Regression, Random Forest Regressor,
and Gradient Boosting, along with Multi-Layer Perceptron Neural Networks.
However, the initial experiments were not at all successful, as the features
were too imbalanced, thus the predictions were not accurate.

After discussions about this issue with the production team and the IT
department, we decided to focus on predicting the quality of the cable coils,
which was the main goal of the project and discard the initial two experiments,
which only tested the feasibility of predicting the value of the some desired
features.

\textbf{Experiment 1: Predicting Value ΟΞΕΙΔΙΑ}
\begin{itemize}
    \item Initial Goal: Predict the value of ΟΞΕΙΔΙΑ.
    \item Limitations: Highly skewed target values.
    \item Outcome: Low accuracy led to further exploration.
\end{itemize}

\textbf{Experiment 2: Predicting a Categorical Label}
\begin{itemize}
    \item Goal: Predict a categorical label suggested by the clients.
    \item Limitations: Class imbalance.
    \item Results: Insufficient data to determine the significance of features for
          prediction.
\end{itemize}

\textbf{Experiment 3: Predicting Quality}
\begin{itemize}
    \item Goal: Predict Quality of Cable Coils.
    \item Limitations: Class imbalance.
    \item Results: No evident differences in class prediction.
\end{itemize}

\textbf{Experiment 4: Binary Classification}
\begin{itemize}
    \item Approach: Classify as `Good` \emph{(1st)} label and `Not Good` \emph{(all
              labels excet the 1st and 2nd)} and drop the \emph{2nd} label quality coils due
          to ambiguity with `Good`, as they were barely distinguishable.
    \item Limitations: Class imbalance.
    \item Rationale: Refined previous experiments based on insights gained.
\end{itemize}

\section{Grid Search and Cross-Validation}

After the initial experiments, me and my collegue focused together on binary
classification to predict the quality of the produced cable coils, where this
time, after communicating with the production team and the IT department, we
included the \emph{2nd} label of quality, so no coils would be left out. We
implemented grid search and cross-validation to optimize model performance.
Several classification algorithms were used, including \emph{XGBClassifier},
\emph{HistGradientBoostingClassifier}, \emph{LGBMClassifier}, and
\emph{CatBoostClassifier}, by utilizing the \emph{sklearn} and
\emph{imbalanced-learn} libraries for the models. We employed
\emph{TimeSeriesSplit} cross-validation to respect the temporal order of the
data and prevent data leakage. Grid search was used for hyperparameter tuning
to find the best-performing model. Supplementing the grid search for the
optimal hyperparameters, we used \emph{Optuna} to further optimize the
hyperparameters of the models and we also applied Grid Search for many scaling
techniques, such as \emph{StandardScaler}, \emph{MinMaxScaler},
\emph{RobustScaler}, and \emph{PowerTransformer}, along with many sampling
techniques, such as \emph{SMOTE}, \emph{SMOTEENN}, and \emph{SMOTETomek}. The
models were evaluated based on various metrics, including accuracy, precision,
recall, F1-score, and AUC-ROC score.

The best-performing model was selected based on the AUC score, which provided a
comprehensive evaluation of the model's performance across different
thresholds, which also accounted for the class imbalance in the dataset, since
the AUC score is not affected by class imbalance like accuracy is. After
developing the models, we used \emph{SHAP} to explain the model predictions and
identify the key features that influenced the quality of the cable coils. For
the evaluation of the results, the interpretability of the model was crucial,
as it provided valuable insights into the factors affecting coil quality which
could be used to improve the production process. Hence, the interpretability of
the \emph{SHAP} diagram was crucial for the stakeholders to understand the
model's predictions and make informed decisions based on the insights provided
and that is the reason why more complex techniques such as Neural Networks were
not used, as they are not as interpretable as the other models used. Below, we
can see the Experiments that were conducted, in the Grid Search and
Cross-Validation phase.

\textbf{Experiment 5: Multiclass Classification}
\begin{itemize}
    \item Tried predicting all qualities present in the validation dataset.
    \item Despite oversampling with SMOTE, the imbalanced dataset presented challenges in
          predicting minority classes, leading to poor performance, thus we decided to
          focus on binary classification.
\end{itemize}

\textbf{Experiment 6: Binary Classification with Grid Search and Time Series Cross-Validation}
\begin{itemize}
    \item Evaluated models on validation data from April 2024.
    \item Used SHAP to explain model predictions.
\end{itemize}

\textbf{Experiment 7: Including Quality of \emph{2nd} label in Binary Classification}
\begin{itemize}
    \item Included coils with the \emph{2nd} label quality in the `good` category.
    \item Improved model performance.
\end{itemize}

\textbf{Experiment 8: Applying Business Rule provided by the client}
\begin{itemize}
    \item Changed model's prediction to `bad` if the two previous coils were predicted as
          `bad`.
    \item Improved predictive performance even further.
\end{itemize}

\section{Validation}

For the validation phase, we applied the same preprocessing steps used for the
training dataset. However, we only retained the selected features chosen during
the training dataset preprocessing. Invalid values were removed to ensure the
quality of the validation dataset. Moreover, the validation dataset contained
coils produced in April 2024, which were not used for training the models. The
validation dataset had a shape of 53.226 rows and 205 columns, with 559 unique
CoilNums. The value counts for binary classification were 423 `Good` and 91
`Bad` coils. The validation dataset was necessary to not include data that the
models had seen before, in order to effectively validate the models to ensure
their accuracy and reliability in predicting the quality of the produced cable
coils.

After all the previous experiments, we conducted the final experiment, which
was the most successful one, where we included the quality of the previous two
coils as additional features, which significantly improved model performance,
achieving an AUC score of 76\%, with the best-performing model being the
\emph{CatBoostClassifier}. This approach leveraged the temporal nature of the
data and captured the sequential relationship between coils, which was crucial
for predicting coil quality accurately. The final model was able to predict the
quality of the produced cable coils with high accuracy, considering the
imbalanced nature of the dataset and the temporal dependencies between coils.
The model was able to identify some key indicators of coil quality such as the
variance as we had previously observed during the EDA phase, which was crucial
for improving the production process and reducing costs.

\section{Conclusion}

The project successfully developed and delivered predictive models that
accurately forecasted the quality of produced cable coils. Key achievements
included implementing advanced data preprocessing and feature engineering
techniques, creating comprehensive data visualizations and reports, and using
model explainability techniques to derive meaningful insights. Incorporating
the quality of previous coils as additional features significantly improved
model performance at the end, leaving the client with a highly accurate and
reliable AI-powered solution for predicting coil quality, that was later also
presented at an energy conference in Athens, Greece in May 2024.

Overall, the project provided valuable experience in applying data science and
machine learning techniques to solve real-world problems, contributing to both
my professional growth and the client's operational improvements.
